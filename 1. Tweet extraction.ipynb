{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sooyoungkim/Desktop/dta_inc/dta'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- 1. Load libraries\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tweepy\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_rows', 1000000)\n",
    "pd.set_option('display.max_columns', 1000000)\n",
    "pd.set_option('display.width', 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 2a. Tweet Extraction Setting\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "COLS = ['id', \n",
    "        'created_at', \n",
    "        'source', \n",
    "        'original_text',\n",
    "        'clean_text', \n",
    "        'sentiment',\n",
    "        'polarity',\n",
    "        'subjectivity', \n",
    "        'lang',\n",
    "        'favorite_count', \n",
    "        'retweet_count', \n",
    "        'original_author',  \n",
    "        'possibly_sensitive', \n",
    "        'hashtags',\n",
    "        'user_mentions', \n",
    "        'place', \n",
    "        'place_coord_boundaries']\n",
    "\n",
    "start_date = '2020-02-01'\n",
    "end_date = '2020-04-01'\n",
    "\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"\n",
    "         u\"\\U0001F300-\\U0001F5FF\"\n",
    "         u\"\\U0001F680-\\U0001F6FF\"\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    tweet = re.sub('\\d+', '', tweet) # remove numbers\n",
    "    tweet = re.sub('‚Ä¶', '', tweet)\n",
    "    tweet = re.sub('[^\\x00-\\x7F]+',' ', tweet)\n",
    "    tweet = re.sub('#',' ', tweet)\n",
    "    tweet = re.sub('RT',' ', tweet)\n",
    "    tweet = re.sub('https',' ', tweet)\n",
    "    tweet = re.sub('[^\\w\\s]',' ', tweet) # remove punctuation\n",
    "    tweet = re.sub('\\n',' ', tweet)\n",
    "    \n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 2b. Tweet Extraction Cleaning\n",
    "\n",
    "def write_tweets(keyword, file):\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, header=0)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=COLS)\n",
    "    for page in tweepy.Cursor(api.search, q=keyword,\n",
    "                              count=1500, include_rts=False, since=start_date).pages(50):\n",
    "        for status in page:\n",
    "            new_entry = []\n",
    "            status = status._json\n",
    "\n",
    "            if status['lang'] != 'en':\n",
    "                continue\n",
    "\n",
    "            if status['created_at'] in df['created_at'].values:\n",
    "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
    "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
    "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                continue\n",
    "\n",
    "            #clean_text = p.clean(status['text'])\n",
    "\n",
    "            filtered_tweet = clean_tweets(status['text'])\n",
    "\n",
    "            blob = TextBlob(filtered_tweet)\n",
    "            Sentiment = blob.sentiment\n",
    "\n",
    "            polarity = Sentiment.polarity\n",
    "            subjectivity = Sentiment.subjectivity\n",
    "\n",
    "            new_entry += [status['id'], status['created_at'],\n",
    "                          status['source'], status['text'],filtered_tweet, Sentiment, polarity, subjectivity, status['lang'],\n",
    "                          status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "            new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "            try:\n",
    "                is_sensitive = status['possibly_sensitive']\n",
    "            except KeyError:\n",
    "                is_sensitive = None\n",
    "            new_entry.append(is_sensitive)\n",
    "\n",
    "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "            new_entry.append(hashtags)\n",
    "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "            new_entry.append(mentions)\n",
    "\n",
    "            try:\n",
    "                location = status['user']['location']\n",
    "            except TypeError:\n",
    "                location = ''\n",
    "            new_entry.append(location)\n",
    "\n",
    "            try:\n",
    "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
    "            except TypeError:\n",
    "                coordinates = None\n",
    "            new_entry.append(coordinates)\n",
    "\n",
    "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
    "            df = df.append(single_tweet_df, ignore_index=True)\n",
    "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
    "    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# ----- 2c. Call tweet extraction\n",
    "write_tweets(hme_keywords, hme_tweets)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
